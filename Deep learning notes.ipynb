{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep learning notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes for later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Review backward and forward propagation - done\n",
    "- Look into dot product and its geometrical interpretationss\n",
    "- Look into keras\n",
    "- Check the difference between tensoflow.keras and keras\n",
    "- find an actual dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Supervised learning__ occurs when your deep learning model learns and makes inferences from data that has already been labeled \n",
    "\n",
    "__Unsupervised learning__ occurs when the model learns and makes inferences from unlabeled data \n",
    "\n",
    "__Artificial neural networks__ are deep learning models that are based on the structure of the brain's neural networks. Same as neural net, net, and model\n",
    "\n",
    "An __activation function__ of a neuron defines the output of that neuron given a set of inputs\n",
    "\n",
    "__Relu__ - rectified linear unit ($\\max(0,x)$)\n",
    "\n",
    "__Sigmoid activation function__ - $\\cfrac{1}{1 + e^{-x}}$\n",
    "\n",
    "__Learning__ is about finding the right weights and biases\n",
    "\n",
    "__Cost(loss) function__ - function that maps an event or values of one or more variables onto a real number, representing from \"cost\" associated with the event. We are seeking to minimize the cost function\n",
    "\n",
    "__Gradient descent__ - first-order iterative optimization algorithm for finding a local minimum of a differentiable function\n",
    "\n",
    "__epoch__ - a single pass of data through the model. The data will be passed through multiple epochs.\n",
    "\n",
    "__SGD (Stochastic Gradient Descent)__ - a type of gradient descent. A few samples are selected randomly instead of the whole dataset for each iteration\n",
    "\n",
    "__The loss function__ is what the gradient descent algorithm is trying to minimize. It is the \"distance\"/error from the actual to computer results\n",
    "\n",
    "__Mean Squared Error (MSE)__ - a common loss function. Here, we get the error by taking the difference between the value the model predicted and the correct label. The formula is given by:\n",
    "$$\\cfrac{\\sum e_i^2}{n}$$\n",
    "where $e_i$ is the error on ith category and n is the total number of categories\n",
    "\n",
    "__Learning rate__ - the number we scale the gradient by. Can be thought of as stepsize\n",
    "\n",
    "__Training data__ - used to train the data. The hope is that the data is general enough so we can use it to predict on new data.  \n",
    "\n",
    "__Validation set__ - used to validate our model during training. Helps give information that can assist with adjusting hyper parameters. Prevents overfitting.\n",
    "\n",
    "__Test set__ - used to test the final model obtained from the training and validation sets. It should not be labeled.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neurons are organized in layers\n",
    "\t- Input layer\n",
    "\t- Hidden layer\n",
    "\t- Output layer\n",
    "Each node is a neuron\n",
    "Each vertical line is a layer\n",
    "The hidden layers are between input and output layers\n",
    "\n",
    "How do you build one?\n",
    "\n",
    "With Keras!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "\n",
    "model = Sequential([Dense(32, input_shape = (10, ), activation = \"relu\"), \n",
    "                    Dense(2, activation = \"softmax\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dense is the most basic type of layer. It connects each input to each  output. The first parameter is the number of nodes in the layer.\n",
    "Some commonly used layers:\n",
    "- Dense (or fully connected) - connects each input to each output\n",
    "- Concolutional layers - image data\n",
    "- Pooling layers\n",
    "- Recurrent layers - time series data\n",
    "- Normalization layers\n",
    "- Many others\n",
    "\n",
    "Each connection transfers the output from the previous layer as input to the receiving unit. Each connection has assigned weight. The output is the weighted sum of the inputs. \n",
    "\n",
    "Output - the classification categories\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from keras.models import Sequential \n",
    "from keras.layers import Dense, Activation\n",
    "\n",
    "model = Sequential([ Dense(5, input_shape = (3,), activation = \"relu\"), \n",
    "                   Dense(2, activation = \"softmax\"),\n",
    "                   ])\n",
    "# Note that the input shape for layers past the first one is not required\n",
    "# because "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the activation function, the neuron is either fired or not fired. Where 0 if for not firing while 1 is for firing.\n",
    "\n",
    "However, an activation function is not always returning a value between 0 and 1. For example, the most widely used activation function - relu. The main idea is that the more positive neuron, the more active it is. \n",
    "\n",
    "Another way to define a sequential model is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(5, input_shape = (3, )))\n",
    "model.add(Activation(\"relu\"))\n",
    "# Here, the activation layer is added separately from the Desnse layer.\n",
    "# The process is the same though"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  The initial bias and weights are randomized. Then, the cost function is defined to tell the computer how far the output is from the expected one. Then, the cost function is optimized. \n",
    " \n",
    " For example, we can have a sum of the squares of differences between the expected and the observed results. \n",
    " \n",
    " !! REMEMBER !!\n",
    " \n",
    " The gradient is pointing in the direction of largest increase. Hence, as we are looking to minimize the cost function, we will be adjusting the parameters in the direction opposite of the gradient. This process is called gradient descent. \n",
    " \n",
    " The value of the gradient is multiplied by the learning rate which is a small number between 0.01 and 0.0001. This is by how much the weights are adjusted with each iteration. When the value is set too high, we are at risk of overshooting. If it's too low, on the other hand, the time to reach the minimum will be much larger.\n",
    " \n",
    " Why use SGD? \n",
    " Using Batch Gradient Descent (the whole dataset is taken), reduces the amount of noise and randomness. However, the problems come up when the datasets are too big. In that case, using all data entries becomes very computationally expensive. Here, the SGD is used: a batch of size 1 is selected for each iteration. The sample is randomly shuffled and selected for the iteration.\n",
    " \n",
    " Since only a single entry is used, the path taken by the algorithm is much noisier. While it usually takes more iterations to reach the minima with SGD, it's still much computationally less expensive compared to batch gradient descent. \n",
    " \n",
    " source: https://www.geeksforgeeks.org/ml-stochastic-gradient-descent-sgd/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.metrics import categorical_crossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Dense(16, input_shape = (1,), activation = \"relu\"), \n",
    "    Dense(32, activation = \"relu\"),\n",
    "    Dense(2, activation = \"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(Adam(learning_rate = 0.0001), \n",
    "              loss = \"sparse_categorical_crossentropy\", \n",
    "              metrics = [\"accuracy\"])\n",
    "#the first parameter is the optimizer. In this case it's Adam,\n",
    "# which is a type of SGD\n",
    "# The second parameter is the learning rate\n",
    "# You can also set the loss by \n",
    "model.loss = \"sparse_categorical_crossentropy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'scaled_train_samples' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-aaba0a6b47f2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m model.fit(scaled_train_samples, train_labels, \n\u001b[0m\u001b[1;32m      2\u001b[0m           \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m          verbose = 2)\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# This is the function that actually trains the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# The first parameter is the training data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'scaled_train_samples' is not defined"
     ]
    }
   ],
   "source": [
    "model.fit(scaled_train_samples, train_labels, \n",
    "          batch_size = 10, epochs = 20, shuffle = True,\n",
    "         verbose = 2)\n",
    "# This is the function that actually trains the model\n",
    "# The first parameter is the training data\n",
    "# The second parameter contains the labels\n",
    "# Both are in the format of a numpy array\n",
    "# Batch size - how many pieces of data we want \n",
    "# to be sent to the model at once\n",
    "# Epochs - there will be 20 individual passes through the data\n",
    "# Shuffle - the data is shuffled before each epoch\n",
    "\n",
    "# find other data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check or set the learning rate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=1e-04>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.optimizer.lr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " For training and testing purposes, the dataset should be broken down into three distinct datasets:\n",
    " - Training set\n",
    " - Validation set\n",
    " - Test set\n",
    " \n",
    " The model is trained on the training set and simultaneously validated with the validation set. The weights are not updated in the validation step. The main goal of the validation set is to make sure the model is not overfitting the data. If the results in the training set are significantly better than in the validation set, the model is likely overfitting.\n",
    " \n",
    " When creating a model with keras, we do not need to specify a validation set. We can set the validation split parameter which will instruct keras to spilt a certain fraction of data and use it as your validation data. \n",
    " \n",
    " Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'scaled_train_samples' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-05e68a4c5e18>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m model.fit(scaled_train_samples, train_labels, \n\u001b[0m\u001b[1;32m      2\u001b[0m           \u001b[0mvalidation_split\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m           epochs = 20, shuffle = True, verbose = 2)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'scaled_train_samples' is not defined"
     ]
    }
   ],
   "source": [
    "model.fit(scaled_train_samples, train_labels, \n",
    "          validation_split = 0.2, batch_size = 10, \n",
    "          epochs = 20, shuffle = True, verbose = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When adding validation set, we will also get preformance metrics for validation data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The validation set can also be created explicitly using validation_data parameter.\n",
    "For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'scaled_train_samples' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-3e2b5d234443>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m model.fit(scaled_train_samples, train_labels, \n\u001b[0m\u001b[1;32m      2\u001b[0m           \u001b[0mvalidation_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalid_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m           epochs = 20, shuffle = True, verbose = 2)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'scaled_train_samples' is not defined"
     ]
    }
   ],
   "source": [
    "model.fit(scaled_train_samples, train_labels, \n",
    "          validation_data = valid_set, batch_size = 10, \n",
    "          epochs = 20, shuffle = True, verbose = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The test set would be structured the same way the training set is but without the labels. We will be using that set when we cann the predict function on our model. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
