{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep learning notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes for later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Review backward and forward propagation - done\n",
    "- Look into dot product and its geometrical interpretationss\n",
    "- Look into keras\n",
    "- Check the difference between tensoflow.keras and keras\n",
    "- find an actual dataset\n",
    "- How to split into classes without unintentially ordering them?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Supervised learning__ occurs when your deep learning model learns and makes inferences from data that has already been labeled \n",
    "\n",
    "__Unsupervised learning__ occurs when the model learns and makes inferences from unlabeled data \n",
    "\n",
    "__Artificial neural networks__ are deep learning models that are based on the structure of the brain's neural networks. Same as neural net, net, and model\n",
    "\n",
    "An __activation function__ of a neuron defines the output of that neuron given a set of inputs\n",
    "\n",
    "__Relu__ - rectified linear unit ($\\max(0,x)$)\n",
    "\n",
    "__Sigmoid activation function__ - $\\cfrac{1}{1 + e^{-x}}$\n",
    "\n",
    "__Learning__ is about finding the right weights and biases\n",
    "\n",
    "__Cost(loss) function__ - function that maps an event or values of one or more variables onto a real number, representing from \"cost\" associated with the event. We are seeking to minimize the cost function\n",
    "\n",
    "__Gradient descent__ - first-order iterative optimization algorithm for finding a local minimum of a differentiable function\n",
    "\n",
    "__epoch__ - a single pass of data through the model. The data will be passed through multiple epochs.\n",
    "\n",
    "__SGD (Stochastic Gradient Descent)__ - a type of gradient descent. A few samples are selected randomly instead of the whole dataset for each iteration\n",
    "\n",
    "__The loss function__ is what the gradient descent algorithm is trying to minimize. It is the \"distance\"/error from the actual to computer results\n",
    "\n",
    "__Mean Squared Error (MSE)__ - a common loss function. Here, we get the error by taking the difference between the value the model predicted and the correct label. The formula is given by:\n",
    "$$\\cfrac{\\sum e_i^2}{n}$$\n",
    "where $e_i$ is the error on ith category and n is the total number of categories\n",
    "\n",
    "__Learning rate__ - the number we scale the gradient by. Can be thought of as stepsize\n",
    "\n",
    "__Training data__ - used to train the data. The hope is that the data is general enough so we can use it to predict on new data.  \n",
    "\n",
    "__Validation set__ - used to validate our model during training. Helps give information that can assist with adjusting hyper parameters. Prevents overfitting.\n",
    "\n",
    "__Test set__ - used to test the final model obtained from the training and validation sets. It should not be labeled.\n",
    "\n",
    "__Overfitting__ occurs when our model is good at predicting the train data but does not perform well with the test set. That is, the model is unable to generalize well\n",
    "\n",
    "__Data Augmentation__ - the process of creating additional augmented data by reasonably modifying the data in our training set. It allows us to add more data to the training set that's similar to the data we already have but has been reasonably modified. \n",
    "\n",
    "__Dropout__ - the model randomly ignores a subset of nodes in a given layer during training. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neurons are organized in layers\n",
    "\t- Input layer\n",
    "\t- Hidden layer\n",
    "\t- Output layer\n",
    "Each node is a neuron\n",
    "Each vertical line is a layer\n",
    "The hidden layers are between input and output layers\n",
    "\n",
    "How do you build one?\n",
    "\n",
    "With Keras!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "\n",
    "model = Sequential([Dense(32, input_shape = (10, ), activation = \"relu\"), \n",
    "                    Dense(2, activation = \"softmax\")])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>malic_acid</th>\n",
       "      <th>ash</th>\n",
       "      <th>alcalinity_of_ash</th>\n",
       "      <th>magnesium</th>\n",
       "      <th>total_phenols</th>\n",
       "      <th>flavanoids</th>\n",
       "      <th>nonflavanoid_phenols</th>\n",
       "      <th>proanthocyanins</th>\n",
       "      <th>color_intensity</th>\n",
       "      <th>hue</th>\n",
       "      <th>od280/od315_of_diluted_wines</th>\n",
       "      <th>proline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.890088</td>\n",
       "      <td>0.306897</td>\n",
       "      <td>0.662539</td>\n",
       "      <td>0.373333</td>\n",
       "      <td>0.617284</td>\n",
       "      <td>0.682990</td>\n",
       "      <td>0.543307</td>\n",
       "      <td>0.393939</td>\n",
       "      <td>0.357542</td>\n",
       "      <td>0.336923</td>\n",
       "      <td>0.614035</td>\n",
       "      <td>0.8500</td>\n",
       "      <td>0.625000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.887390</td>\n",
       "      <td>0.406897</td>\n",
       "      <td>0.826625</td>\n",
       "      <td>0.620000</td>\n",
       "      <td>0.623457</td>\n",
       "      <td>0.721649</td>\n",
       "      <td>0.637795</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.784916</td>\n",
       "      <td>0.436923</td>\n",
       "      <td>0.602339</td>\n",
       "      <td>0.7925</td>\n",
       "      <td>0.705357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.968982</td>\n",
       "      <td>0.336207</td>\n",
       "      <td>0.773994</td>\n",
       "      <td>0.560000</td>\n",
       "      <td>0.697531</td>\n",
       "      <td>0.992268</td>\n",
       "      <td>0.687008</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>0.608939</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.502924</td>\n",
       "      <td>0.8625</td>\n",
       "      <td>0.880952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.892785</td>\n",
       "      <td>0.446552</td>\n",
       "      <td>0.888545</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.728395</td>\n",
       "      <td>0.721649</td>\n",
       "      <td>0.529528</td>\n",
       "      <td>0.590909</td>\n",
       "      <td>0.508380</td>\n",
       "      <td>0.332308</td>\n",
       "      <td>0.608187</td>\n",
       "      <td>0.7325</td>\n",
       "      <td>0.437500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>0.970330</td>\n",
       "      <td>0.322414</td>\n",
       "      <td>0.758514</td>\n",
       "      <td>0.486667</td>\n",
       "      <td>0.592593</td>\n",
       "      <td>0.644330</td>\n",
       "      <td>0.496063</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.553073</td>\n",
       "      <td>0.403846</td>\n",
       "      <td>0.596491</td>\n",
       "      <td>0.8950</td>\n",
       "      <td>0.767857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>172</td>\n",
       "      <td>0.954821</td>\n",
       "      <td>0.432759</td>\n",
       "      <td>0.767802</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.561728</td>\n",
       "      <td>0.432990</td>\n",
       "      <td>0.137795</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.346369</td>\n",
       "      <td>0.746154</td>\n",
       "      <td>0.362573</td>\n",
       "      <td>0.4275</td>\n",
       "      <td>0.392857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>173</td>\n",
       "      <td>0.924477</td>\n",
       "      <td>0.974138</td>\n",
       "      <td>0.758514</td>\n",
       "      <td>0.683333</td>\n",
       "      <td>0.586420</td>\n",
       "      <td>0.432990</td>\n",
       "      <td>0.120079</td>\n",
       "      <td>0.787879</td>\n",
       "      <td>0.296089</td>\n",
       "      <td>0.592308</td>\n",
       "      <td>0.374269</td>\n",
       "      <td>0.4350</td>\n",
       "      <td>0.440476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>174</td>\n",
       "      <td>0.903574</td>\n",
       "      <td>0.674138</td>\n",
       "      <td>0.767802</td>\n",
       "      <td>0.766667</td>\n",
       "      <td>0.629630</td>\n",
       "      <td>0.463918</td>\n",
       "      <td>0.147638</td>\n",
       "      <td>0.651515</td>\n",
       "      <td>0.393855</td>\n",
       "      <td>0.561538</td>\n",
       "      <td>0.409357</td>\n",
       "      <td>0.3900</td>\n",
       "      <td>0.446429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>176</td>\n",
       "      <td>0.888065</td>\n",
       "      <td>0.446552</td>\n",
       "      <td>0.733746</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.740741</td>\n",
       "      <td>0.425258</td>\n",
       "      <td>0.133858</td>\n",
       "      <td>0.803030</td>\n",
       "      <td>0.407821</td>\n",
       "      <td>0.715385</td>\n",
       "      <td>0.350877</td>\n",
       "      <td>0.4050</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>177</td>\n",
       "      <td>0.952798</td>\n",
       "      <td>0.706897</td>\n",
       "      <td>0.848297</td>\n",
       "      <td>0.816667</td>\n",
       "      <td>0.592593</td>\n",
       "      <td>0.528351</td>\n",
       "      <td>0.149606</td>\n",
       "      <td>0.848485</td>\n",
       "      <td>0.377095</td>\n",
       "      <td>0.707692</td>\n",
       "      <td>0.356725</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>142 rows Ã— 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     index   alcohol  malic_acid       ash  alcalinity_of_ash  magnesium  \\\n",
       "0        1  0.890088    0.306897  0.662539           0.373333   0.617284   \n",
       "1        2  0.887390    0.406897  0.826625           0.620000   0.623457   \n",
       "2        3  0.968982    0.336207  0.773994           0.560000   0.697531   \n",
       "3        4  0.892785    0.446552  0.888545           0.700000   0.728395   \n",
       "4        6  0.970330    0.322414  0.758514           0.486667   0.592593   \n",
       "..     ...       ...         ...       ...                ...        ...   \n",
       "137    172  0.954821    0.432759  0.767802           0.666667   0.561728   \n",
       "138    173  0.924477    0.974138  0.758514           0.683333   0.586420   \n",
       "139    174  0.903574    0.674138  0.767802           0.766667   0.629630   \n",
       "140    176  0.888065    0.446552  0.733746           0.666667   0.740741   \n",
       "141    177  0.952798    0.706897  0.848297           0.816667   0.592593   \n",
       "\n",
       "     total_phenols  flavanoids  nonflavanoid_phenols  proanthocyanins  \\\n",
       "0         0.682990    0.543307              0.393939         0.357542   \n",
       "1         0.721649    0.637795              0.454545         0.784916   \n",
       "2         0.992268    0.687008              0.363636         0.608939   \n",
       "3         0.721649    0.529528              0.590909         0.508380   \n",
       "4         0.644330    0.496063              0.454545         0.553073   \n",
       "..             ...         ...                   ...              ...   \n",
       "137       0.432990    0.137795              0.666667         0.346369   \n",
       "138       0.432990    0.120079              0.787879         0.296089   \n",
       "139       0.463918    0.147638              0.651515         0.393855   \n",
       "140       0.425258    0.133858              0.803030         0.407821   \n",
       "141       0.528351    0.149606              0.848485         0.377095   \n",
       "\n",
       "     color_intensity       hue  od280/od315_of_diluted_wines   proline  \n",
       "0           0.336923  0.614035                        0.8500  0.625000  \n",
       "1           0.436923  0.602339                        0.7925  0.705357  \n",
       "2           0.600000  0.502924                        0.8625  0.880952  \n",
       "3           0.332308  0.608187                        0.7325  0.437500  \n",
       "4           0.403846  0.596491                        0.8950  0.767857  \n",
       "..               ...       ...                           ...       ...  \n",
       "137         0.746154  0.362573                        0.4275  0.392857  \n",
       "138         0.592308  0.374269                        0.4350  0.440476  \n",
       "139         0.561538  0.409357                        0.3900  0.446429  \n",
       "140         0.715385  0.350877                        0.4050  0.500000  \n",
       "141         0.707692  0.356725                        0.4000  0.333333  \n",
       "\n",
       "[142 rows x 14 columns]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sklearn.datasets\n",
    "\n",
    "data = sklearn.datasets.load_wine(as_frame = True)['data']\n",
    "labels = sklearn.datasets.load_wine(as_frame = True)['target'][data.index % 5 != 0].reset_index()\n",
    "\n",
    "for i in data.columns:\n",
    "    data[i] = data[i].divide(data[i].max())\n",
    "train = data[data.index % 5 != 0].reset_index()\n",
    "test = data[data.index % 5 == 0].reset_index()\n",
    "labels\n",
    "train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dense is the most basic type of layer. It connects each input to each  output. The first parameter is the number of nodes in the layer.\n",
    "Some commonly used layers:\n",
    "- Dense (or fully connected) - connects each input to each output\n",
    "- Concolutional layers - image data\n",
    "- Pooling layers\n",
    "- Recurrent layers - time series data\n",
    "- Normalization layers\n",
    "- Many others\n",
    "\n",
    "Each connection transfers the output from the previous layer as input to the receiving unit. Each connection has assigned weight. The output is the weighted sum of the inputs. \n",
    "\n",
    "Output - the classification categories\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from keras.models import Sequential \n",
    "from keras.layers import Dense, Activation\n",
    "\n",
    "model = Sequential([ Dense(5, input_shape = (13,), activation = \"relu\"), \n",
    "                   Dense(2, activation = \"softmax\"),\n",
    "                   ])\n",
    "# Note that the input shape for layers past the first one is not required\n",
    "# because "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the activation function, the neuron is either fired or not fired. Where 0 if for not firing while 1 is for firing.\n",
    "\n",
    "However, an activation function is not always returning a value between 0 and 1. For example, the most widely used activation function - relu. The main idea is that the more positive neuron, the more active it is. \n",
    "\n",
    "Another way to define a sequential model is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(5, input_shape = (13, )))\n",
    "model.add(Activation(\"relu\"))\n",
    "# Here, the activation layer is added separately from the Desnse layer.\n",
    "# The process is the same though"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  The initial bias and weights are randomized. Then, the cost function is defined to tell the computer how far the output is from the expected one. Then, the cost function is optimized. \n",
    " \n",
    " For example, we can have a sum of the squares of differences between the expected and the observed results. \n",
    " \n",
    " !! REMEMBER !!\n",
    " \n",
    " The gradient is pointing in the direction of largest increase. Hence, as we are looking to minimize the cost function, we will be adjusting the parameters in the direction opposite of the gradient. This process is called gradient descent. \n",
    " \n",
    " The value of the gradient is multiplied by the learning rate which is a small number between 0.01 and 0.0001. This is by how much the weights are adjusted with each iteration. When the value is set too high, we are at risk of overshooting. If it's too low, on the other hand, the time to reach the minimum will be much larger.\n",
    " \n",
    " Why use SGD? \n",
    " Using Batch Gradient Descent (the whole dataset is taken), reduces the amount of noise and randomness. However, the problems come up when the datasets are too big. In that case, using all data entries becomes very computationally expensive. Here, the SGD is used: a batch of size 1 is selected for each iteration. The sample is randomly shuffled and selected for the iteration.\n",
    " \n",
    " Since only a single entry is used, the path taken by the algorithm is much noisier. While it usually takes more iterations to reach the minima with SGD, it's still much computationally less expensive compared to batch gradient descent. \n",
    " \n",
    " source: https://www.geeksforgeeks.org/ml-stochastic-gradient-descent-sgd/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.metrics import categorical_crossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Dense(10, input_shape = (13,), activation = \"relu\"), \n",
    "    Dense(8, activation = \"relu\"),\n",
    "    Dense(3, activation = \"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(Adam(learning_rate = 0.001), \n",
    "              loss = \"sparse_categorical_crossentropy\", \n",
    "              metrics = [\"accuracy\"])\n",
    "#the first parameter is the optimizer. In this case it's Adam,\n",
    "# which is a type of SGD\n",
    "# The second parameter is the learning rate\n",
    "# You can also set the loss by \n",
    "model.loss = \"sparse_categorical_crossentropy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "15/15 - 1s - loss: 1.1803 - accuracy: 0.1333 - 518ms/epoch - 35ms/step\n",
      "Epoch 2/20\n",
      "15/15 - 0s - loss: 1.1034 - accuracy: 0.4000 - 34ms/epoch - 2ms/step\n",
      "Epoch 3/20\n",
      "15/15 - 0s - loss: 1.0786 - accuracy: 0.5200 - 46ms/epoch - 3ms/step\n",
      "Epoch 4/20\n",
      "15/15 - 0s - loss: 1.0673 - accuracy: 0.5133 - 29ms/epoch - 2ms/step\n",
      "Epoch 5/20\n",
      "15/15 - 0s - loss: 1.0564 - accuracy: 0.5467 - 34ms/epoch - 2ms/step\n",
      "Epoch 6/20\n",
      "15/15 - 0s - loss: 1.0447 - accuracy: 0.6267 - 45ms/epoch - 3ms/step\n",
      "Epoch 7/20\n",
      "15/15 - 0s - loss: 1.0337 - accuracy: 0.7200 - 41ms/epoch - 3ms/step\n",
      "Epoch 8/20\n",
      "15/15 - 0s - loss: 1.0235 - accuracy: 0.7333 - 45ms/epoch - 3ms/step\n",
      "Epoch 9/20\n",
      "15/15 - 0s - loss: 1.0135 - accuracy: 0.7533 - 56ms/epoch - 4ms/step\n",
      "Epoch 10/20\n",
      "15/15 - 0s - loss: 1.0033 - accuracy: 0.7800 - 49ms/epoch - 3ms/step\n",
      "Epoch 11/20\n",
      "15/15 - 0s - loss: 0.9922 - accuracy: 0.7867 - 30ms/epoch - 2ms/step\n",
      "Epoch 12/20\n",
      "15/15 - 0s - loss: 0.9803 - accuracy: 0.8067 - 40ms/epoch - 3ms/step\n",
      "Epoch 13/20\n",
      "15/15 - 0s - loss: 0.9680 - accuracy: 0.8067 - 45ms/epoch - 3ms/step\n",
      "Epoch 14/20\n",
      "15/15 - 0s - loss: 0.9542 - accuracy: 0.8067 - 49ms/epoch - 3ms/step\n",
      "Epoch 15/20\n",
      "15/15 - 0s - loss: 0.9391 - accuracy: 0.8133 - 41ms/epoch - 3ms/step\n",
      "Epoch 16/20\n",
      "15/15 - 0s - loss: 0.9241 - accuracy: 0.8133 - 47ms/epoch - 3ms/step\n",
      "Epoch 17/20\n",
      "15/15 - 0s - loss: 0.9066 - accuracy: 0.8133 - 48ms/epoch - 3ms/step\n",
      "Epoch 18/20\n",
      "15/15 - 0s - loss: 0.8896 - accuracy: 0.8067 - 143ms/epoch - 10ms/step\n",
      "Epoch 19/20\n",
      "15/15 - 0s - loss: 0.8708 - accuracy: 0.8067 - 41ms/epoch - 3ms/step\n",
      "Epoch 20/20\n",
      "15/15 - 0s - loss: 0.8515 - accuracy: 0.8133 - 44ms/epoch - 3ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f8c2d05fe20>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train, labels, \n",
    "          batch_size = 10, epochs = 20, shuffle = True,\n",
    "         verbose = 2)\n",
    "# This is the function that actually trains the model\n",
    "# The first parameter is the training data\n",
    "# The second parameter contains the labels\n",
    "# Both are in the format of a numpy array\n",
    "# Batch size - how many pieces of data we want \n",
    "# to be sent to the model at once\n",
    "# Epochs - there will be 20 individual passes through the data\n",
    "# Shuffle - the data is shuffled before each epoch\n",
    "\n",
    "# find other data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check or set the learning rate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.optimizer.lr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " For training and testing purposes, the dataset should be broken down into three distinct datasets:\n",
    " - Training set\n",
    " - Validation set\n",
    " - Test set\n",
    " \n",
    " The model is trained on the training set and simultaneously validated with the validation set. The weights are not updated in the validation step. The main goal of the validation set is to make sure the model is not overfitting the data. If the results in the training set are significantly better than in the validation set, the model is likely overfitting.\n",
    " \n",
    " When creating a model with keras, we do not need to specify a validation set. We can set the validation split parameter which will instruct keras to spilt a certain fraction of data and use it as your validation data. \n",
    " \n",
    " Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "12/12 - 0s - loss: 0.7794 - accuracy: 0.9333 - val_loss: 1.0791 - val_accuracy: 0.2667 - 195ms/epoch - 16ms/step\n",
      "Epoch 2/20\n",
      "12/12 - 0s - loss: 0.7506 - accuracy: 0.9083 - val_loss: 1.1317 - val_accuracy: 0.2333 - 53ms/epoch - 4ms/step\n",
      "Epoch 3/20\n",
      "12/12 - 0s - loss: 0.7234 - accuracy: 0.9000 - val_loss: 1.1508 - val_accuracy: 0.2333 - 55ms/epoch - 5ms/step\n",
      "Epoch 4/20\n",
      "12/12 - 0s - loss: 0.6956 - accuracy: 0.9083 - val_loss: 1.1675 - val_accuracy: 0.2667 - 48ms/epoch - 4ms/step\n",
      "Epoch 5/20\n",
      "12/12 - 0s - loss: 0.6677 - accuracy: 0.9250 - val_loss: 1.1892 - val_accuracy: 0.2667 - 53ms/epoch - 4ms/step\n",
      "Epoch 6/20\n",
      "12/12 - 0s - loss: 0.6405 - accuracy: 0.9333 - val_loss: 1.1930 - val_accuracy: 0.2667 - 55ms/epoch - 5ms/step\n",
      "Epoch 7/20\n",
      "12/12 - 0s - loss: 0.6147 - accuracy: 0.9500 - val_loss: 1.2245 - val_accuracy: 0.2667 - 76ms/epoch - 6ms/step\n",
      "Epoch 8/20\n",
      "12/12 - 0s - loss: 0.5852 - accuracy: 0.9333 - val_loss: 1.2697 - val_accuracy: 0.2667 - 95ms/epoch - 8ms/step\n",
      "Epoch 9/20\n",
      "12/12 - 0s - loss: 0.5594 - accuracy: 0.9333 - val_loss: 1.2887 - val_accuracy: 0.2667 - 63ms/epoch - 5ms/step\n",
      "Epoch 10/20\n",
      "12/12 - 0s - loss: 0.5361 - accuracy: 0.9417 - val_loss: 1.3008 - val_accuracy: 0.2667 - 73ms/epoch - 6ms/step\n",
      "Epoch 11/20\n",
      "12/12 - 0s - loss: 0.5077 - accuracy: 0.9500 - val_loss: 1.3399 - val_accuracy: 0.2667 - 62ms/epoch - 5ms/step\n",
      "Epoch 12/20\n",
      "12/12 - 0s - loss: 0.4874 - accuracy: 0.9500 - val_loss: 1.3816 - val_accuracy: 0.2667 - 60ms/epoch - 5ms/step\n",
      "Epoch 13/20\n",
      "12/12 - 0s - loss: 0.4600 - accuracy: 0.9500 - val_loss: 1.3995 - val_accuracy: 0.2667 - 51ms/epoch - 4ms/step\n",
      "Epoch 14/20\n",
      "12/12 - 0s - loss: 0.4388 - accuracy: 0.9583 - val_loss: 1.4351 - val_accuracy: 0.2667 - 66ms/epoch - 6ms/step\n",
      "Epoch 15/20\n",
      "12/12 - 0s - loss: 0.4173 - accuracy: 0.9583 - val_loss: 1.4793 - val_accuracy: 0.2667 - 51ms/epoch - 4ms/step\n",
      "Epoch 16/20\n",
      "12/12 - 0s - loss: 0.3974 - accuracy: 0.9500 - val_loss: 1.5290 - val_accuracy: 0.2667 - 61ms/epoch - 5ms/step\n",
      "Epoch 17/20\n",
      "12/12 - 0s - loss: 0.3786 - accuracy: 0.9500 - val_loss: 1.5592 - val_accuracy: 0.3000 - 85ms/epoch - 7ms/step\n",
      "Epoch 18/20\n",
      "12/12 - 0s - loss: 0.3604 - accuracy: 0.9583 - val_loss: 1.6038 - val_accuracy: 0.3000 - 56ms/epoch - 5ms/step\n",
      "Epoch 19/20\n",
      "12/12 - 0s - loss: 0.3441 - accuracy: 0.9583 - val_loss: 1.6505 - val_accuracy: 0.2667 - 73ms/epoch - 6ms/step\n",
      "Epoch 20/20\n",
      "12/12 - 0s - loss: 0.3324 - accuracy: 0.9500 - val_loss: 1.6915 - val_accuracy: 0.3000 - 59ms/epoch - 5ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f8c2c7a5880>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train, labels, \n",
    "          validation_split = 0.2, batch_size = 10, \n",
    "          epochs = 20, shuffle = True, verbose = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When adding validation set, we will also get preformance metrics for validation data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The validation set can also be created explicitly using validation_data parameter.\n",
    "For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "12/12 - 0s - loss: 0.6321 - accuracy: 0.8250 - val_loss: 0.6317 - val_accuracy: 0.8333 - 111ms/epoch - 9ms/step\n",
      "Epoch 2/20\n",
      "12/12 - 0s - loss: 0.6074 - accuracy: 0.8333 - val_loss: 0.6064 - val_accuracy: 0.8333 - 48ms/epoch - 4ms/step\n",
      "Epoch 3/20\n",
      "12/12 - 0s - loss: 0.5794 - accuracy: 0.8333 - val_loss: 0.5811 - val_accuracy: 0.8333 - 61ms/epoch - 5ms/step\n",
      "Epoch 4/20\n",
      "12/12 - 0s - loss: 0.5427 - accuracy: 0.8333 - val_loss: 0.5534 - val_accuracy: 0.8333 - 77ms/epoch - 6ms/step\n",
      "Epoch 5/20\n",
      "12/12 - 0s - loss: 0.5090 - accuracy: 0.8333 - val_loss: 0.5242 - val_accuracy: 0.8333 - 83ms/epoch - 7ms/step\n",
      "Epoch 6/20\n",
      "12/12 - 0s - loss: 0.4793 - accuracy: 0.8333 - val_loss: 0.5006 - val_accuracy: 0.8333 - 78ms/epoch - 7ms/step\n",
      "Epoch 7/20\n",
      "12/12 - 0s - loss: 0.4584 - accuracy: 0.8417 - val_loss: 0.4841 - val_accuracy: 0.8333 - 74ms/epoch - 6ms/step\n",
      "Epoch 8/20\n",
      "12/12 - 0s - loss: 0.4364 - accuracy: 0.8417 - val_loss: 0.4746 - val_accuracy: 0.8333 - 66ms/epoch - 5ms/step\n",
      "Epoch 9/20\n",
      "12/12 - 0s - loss: 0.4273 - accuracy: 0.8333 - val_loss: 0.4657 - val_accuracy: 0.8333 - 55ms/epoch - 5ms/step\n",
      "Epoch 10/20\n",
      "12/12 - 0s - loss: 0.4132 - accuracy: 0.8333 - val_loss: 0.4552 - val_accuracy: 0.8333 - 60ms/epoch - 5ms/step\n",
      "Epoch 11/20\n",
      "12/12 - 0s - loss: 0.4046 - accuracy: 0.8333 - val_loss: 0.4514 - val_accuracy: 0.8333 - 69ms/epoch - 6ms/step\n",
      "Epoch 12/20\n",
      "12/12 - 0s - loss: 0.3960 - accuracy: 0.8333 - val_loss: 0.4458 - val_accuracy: 0.8333 - 83ms/epoch - 7ms/step\n",
      "Epoch 13/20\n",
      "12/12 - 0s - loss: 0.3878 - accuracy: 0.8333 - val_loss: 0.4378 - val_accuracy: 0.8333 - 88ms/epoch - 7ms/step\n",
      "Epoch 14/20\n",
      "12/12 - 0s - loss: 0.3805 - accuracy: 0.8333 - val_loss: 0.4337 - val_accuracy: 0.8333 - 59ms/epoch - 5ms/step\n",
      "Epoch 15/20\n",
      "12/12 - 0s - loss: 0.3736 - accuracy: 0.8333 - val_loss: 0.4269 - val_accuracy: 0.8333 - 49ms/epoch - 4ms/step\n",
      "Epoch 16/20\n",
      "12/12 - 0s - loss: 0.3660 - accuracy: 0.8333 - val_loss: 0.4242 - val_accuracy: 0.8333 - 65ms/epoch - 5ms/step\n",
      "Epoch 17/20\n",
      "12/12 - 0s - loss: 0.3602 - accuracy: 0.8333 - val_loss: 0.4199 - val_accuracy: 0.8333 - 70ms/epoch - 6ms/step\n",
      "Epoch 18/20\n",
      "12/12 - 0s - loss: 0.3539 - accuracy: 0.8333 - val_loss: 0.4098 - val_accuracy: 0.8333 - 55ms/epoch - 5ms/step\n",
      "Epoch 19/20\n",
      "12/12 - 0s - loss: 0.3438 - accuracy: 0.8333 - val_loss: 0.4083 - val_accuracy: 0.8333 - 56ms/epoch - 5ms/step\n",
      "Epoch 20/20\n",
      "12/12 - 0s - loss: 0.3387 - accuracy: 0.8333 - val_loss: 0.4055 - val_accuracy: 0.8333 - 64ms/epoch - 5ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f8c2d0f2dc0>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "valid_set = train[train.index % 5 == 0]\n",
    "valid_set_labels = labels[labels.index % 5 == 0]\n",
    "train2 = train[train.index % 5 != 0]\n",
    "labels2 = labels[labels.index % 5 != 0]\n",
    "model.fit(train2, labels2,\n",
    "          validation_data = (valid_set, valid_set_labels), batch_size = 10, \n",
    "          epochs = 20, shuffle = True, verbose = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The test set would be structured the same way the training set is but without the labels. We will be using that set when we cann the predict function on our model. \n",
    "\n",
    "We need to make sure our training and validation sets are a good representitive of actual data. \n",
    "\n",
    "To predict with keras:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.13130848, 0.5342713 , 0.3344202 ],\n",
       "       [0.3565282 , 0.37334207, 0.27012974],\n",
       "       [0.02354746, 0.7503886 , 0.226064  ],\n",
       "       [0.37337756, 0.3908902 , 0.23573218],\n",
       "       [0.08957515, 0.65465724, 0.25576752],\n",
       "       [0.18293667, 0.46714225, 0.34992114],\n",
       "       [0.18896735, 0.49854043, 0.31249222],\n",
       "       [0.14090349, 0.5300852 , 0.32901138],\n",
       "       [0.44569394, 0.3899349 , 0.16437116],\n",
       "       [0.400489  , 0.4116825 , 0.18782845],\n",
       "       [0.17960975, 0.48920405, 0.33118623],\n",
       "       [0.092433  , 0.53762347, 0.36994356],\n",
       "       [0.03154162, 0.6732346 , 0.2952238 ],\n",
       "       [0.16717309, 0.4861888 , 0.34663817],\n",
       "       [0.19798373, 0.48694307, 0.3150732 ],\n",
       "       [0.03746299, 0.61912835, 0.34340864],\n",
       "       [0.26895452, 0.4130004 , 0.31804517],\n",
       "       [0.42786053, 0.32802674, 0.24411277],\n",
       "       [0.17660657, 0.531272  , 0.2921214 ],\n",
       "       [0.2577894 , 0.39539802, 0.34681252],\n",
       "       [0.06826068, 0.6129884 , 0.3187509 ],\n",
       "       [0.17096773, 0.5241269 , 0.30490535],\n",
       "       [0.3060906 , 0.4352714 , 0.258638  ],\n",
       "       [0.2865767 , 0.37728068, 0.3361426 ],\n",
       "       [0.18316548, 0.47171676, 0.3451178 ],\n",
       "       [0.55069196, 0.22959775, 0.21971034],\n",
       "       [0.392872  , 0.3594853 , 0.24764265],\n",
       "       [0.10947023, 0.54633594, 0.34419388]], dtype=float32)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = model.predict(test, \n",
    "                            batch_size = 10, verbose = 0)\n",
    "# First is the variable holding the test data\n",
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To know that the model is overfitting: \n",
    "- If the validation metrics are considerably worse than the training metrics than that's an indication of overfitting\n",
    "- The metrics are good during training but accuracy is low on test data\n",
    "\n",
    "Some ways to aviod overfitting:\n",
    "- Add more data. The more data, higher diversity\n",
    "- Data augmentation\n",
    "- Reduce complexity of the model. This can be done by making simple changes such as removing layers from the model or reducing the number of neurons in the layers.\n",
    "- Dropout"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
