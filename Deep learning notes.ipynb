{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep learning notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes for later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Review backward and forward propagation - in the process\n",
    "- Look into dot product and its geometrical interpretationss\n",
    "- Look into keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Supervised learning__ occurs when your deep learning model learns and makes inferences from data that has already been labeled \n",
    "\n",
    "__Unsupervised learning__ occurs when the model learns and makes inferences from unlabeled data \n",
    "\n",
    "__Artificial neural networks__ are deep learning models that are based on the structure of the brain's neural networks. Same as neural net, net, and model\n",
    "\n",
    "An __activation function__ of a neuron defines the output of that neuron given a set of inputs\n",
    "\n",
    "__Relu__ - rectified linear unit ($\\max(0,x)$)\n",
    "\n",
    "__Sigmoid activation function__ - $\\cfrac{1}{1 + e^{-x}}$\n",
    "\n",
    "__Learning__ is about finding the right weights and biases\n",
    "\n",
    "__Cost(loss) function__ - function that maps an event or values of one or more variables onto a real number, representing from \"cost\" associated with the event. We are seeking to minimize the cost function\n",
    "\n",
    "__Gradient descent__ - first-order iterative optimization algorithm for finding a local minimum of a differentiable function\n",
    "\n",
    "__epoch__ - a single pass of data through the model. The data will be passed through multiple epochs.\n",
    "\n",
    "__SGD (Stochastic Gradient Descent)__ - a type of gradient descent. A few samples are selected randomly instead of the whole dataset for each iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neurons are organized in layers\n",
    "\t- Input layer\n",
    "\t- Hidden layer\n",
    "\t- Output layer\n",
    "Each node is a neuron\n",
    "Each vertical line is a layer\n",
    "The hidden layers are between input and output layers\n",
    "\n",
    "How do you build one?\n",
    "\n",
    "With Keras!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "\n",
    "model = Sequential([Dense(32, input_shape = (10, ), activation = \"relu\"), \n",
    "                    Dense(2, activation = \"softmax\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dense is the most basic type of layer. It connects each input to each  output. The first parameter is the number of nodes in the layer.\n",
    "Some commonly used layers:\n",
    "- Dense (or fully connected) - connects each input to each output\n",
    "- Concolutional layers - image data\n",
    "- Pooling layers\n",
    "- Recurrent layers - time series data\n",
    "- Normalization layers\n",
    "- Many others\n",
    "\n",
    "Each connection transfers the output from the previous layer as input to the receiving unit. Each connection has assigned weight. The output is the weighted sum of the inputs. \n",
    "\n",
    "Output - the classification categories\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from keras.models import Sequential \n",
    "from keras.layers import Dense, Activation\n",
    "\n",
    "model = Sequential([ Dense(5, input_shape = (3,), activation = \"relu\"), \n",
    "                   Dense(2, activation = \"softmax\"),\n",
    "                   ])\n",
    "# Note that the input shape for layers past the first one is not required\n",
    "# because "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the activation function, the neuron is either fired or not fired. Where 0 if for not firing while 1 is for firing.\n",
    "\n",
    "However, an activation function is not always returning a value between 0 and 1. For example, the most widely used activation function - relu. The main idea is that the more positive neuron, the more active it is. \n",
    "\n",
    "Another way to define a sequential model is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(5, input_shape = (3, )))\n",
    "model.add(Activation(\"relu\"))\n",
    "# Here, the activation layer is added separately from the Desnse layer.\n",
    "# The process is the same though"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  The initial bias and weights are randomized. Then, the cost function is defined to tell the computer how far the output is from the expected one. Then, the cost function is optimized. \n",
    " \n",
    " For example, we can have a sum of the squares of differences between the expected and the observed results. \n",
    " \n",
    " !! REMEMBER !!\n",
    " \n",
    " The gradient is pointing in the direction of largest increase. Hence, as we are looking to minimize the cost function, we will be adjusting the parameters in the direction opposite of the gradient. This process is called gradient descent. \n",
    " \n",
    " The value of the gradient is multiplied by the learning rate which is a small number between 0.01 and 0.0001. This is by how much the weights are adjusted with each iteration. \n",
    " \n",
    " Why use SGD? \n",
    " Using Batch Gradient Descent (the whole dataset is taken), reduces the amount of noise and randomness. However, the problems come up when the datasets are too big. In that case, using all data entries becomes very computationally expensive. Here, the SGD is used: a batch of size 1 is selected for each iteration. The sample is randomly shuffled and selected for the iteration.\n",
    " \n",
    " Since only a single entry is used, the path taken by the algorithm is much noisier. While it usually takes more iterations to reach the minima with SGD, it's still much computationally less expensive compared to batch gradient descent. \n",
    " \n",
    " source: https://www.geeksforgeeks.org/ml-stochastic-gradient-descent-sgd/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'Adam' from 'keras.optimizers' (/Users/sofyamalashchenko/opt/anaconda3/lib/python3.8/site-packages/keras/optimizers.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-a1bc3c4cc588>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbackend\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAdam\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcategorical_crossentropy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'Adam' from 'keras.optimizers' (/Users/sofyamalashchenko/opt/anaconda3/lib/python3.8/site-packages/keras/optimizers.py)"
     ]
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "from keras.optimizers import Adam\n",
    "from keras.metrics import categorical_crossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Dense(16, input_shape = (1,), activation = \"relu\"), \n",
    "    Dense(32, activation = \"relu\"),\n",
    "    Dense(2, activtion = \"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(Adam(lr = 0.0001), \n",
    "              loss = \"sparese_categorical_crossentropy\", \n",
    "              metrics = [\"accuracy\"])\n",
    "#the first parameter is the optimizer. In this case it's Adam,\n",
    "# which is a type of SGD\n",
    "# The second parameter is the learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-aaba0a6b47f2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m model.fit(scaled_train_samples, train_labels, \n\u001b[0m\u001b[1;32m      2\u001b[0m           \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m          verbose = 2)\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# This is the function that actually trains the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# The first parameter is the training data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "model.fit(scaled_train_samples, train_labels, \n",
    "          batch_size = 10, epochs = 20, shuffle = True,\n",
    "         verbose = 2)\n",
    "# This is the function that actually trains the model\n",
    "# The first parameter is the training data\n",
    "# The second parameter contains the labels\n",
    "# Batch size - how many pieces of data we want \n",
    "# to be sent to the model at once\n",
    "# Epochs - there will be 20 individual passes through the data\n",
    "# Shuffle - the data is shuffled before each epoch\n",
    "\n",
    "# Note that the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
